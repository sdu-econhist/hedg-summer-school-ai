{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Classification of Occupational Descriptions\n",
    "\n",
    "1. Reproduce the HISCO classification model from the slides by downloading and loading the datasets and running the associated code (see [Exercise 1](##exercise-1))\n",
    "1. Try changing the `nn.GRU` layer to an `nn.RNN` layer and then to an `nn.LSTM` layer and train your new models\n",
    "1. Try changing the optimizer to something other than `torch.optim.AdamW`. A list of optimizers is available [here](https://pytorch.org/docs/stable/optim.html#algorithms)\n",
    "1. Experiment with, e.g., the types or numbers of layers in your model, the choice of optimizer and learning rate, or the number of epochs. How high performance can you achieve on the test split?\n",
    "\n",
    "## Data\n",
    "\n",
    "Start by downloading the datasets `toy_data_train.csv` and `toy_data_test.csv`.\n",
    "\n",
    "Alternatively, if you have `histocc` installed, you can run the code below to prepare the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from histocc import DATASETS\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def download_and_prepare_data():\n",
    "    keys = DATASETS['keys']()\n",
    "    mapping = dict(keys[['hisco', 'code']].values)\n",
    "\n",
    "    toydata = DATASETS['toydata']()\n",
    "    toydata['label'] = toydata['hisco_1'].transform(lambda x: mapping[x])\n",
    "\n",
    "    train, test = train_test_split(\n",
    "        toydata[['occ1', 'label']],\n",
    "        test_size=0.1,\n",
    "        random_state=42,\n",
    "        )\n",
    "\n",
    "    train.to_csv('./toy_data_train.csv', index=False)\n",
    "    test.to_csv('./toy_data_test.csv', index=False)\n",
    "\n",
    "\n",
    "download_and_prepare_data()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch import Tensor, nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Dataset` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all the individual tokens present in the toy dataset\n",
    "CHARS_IN_TOYDATA = [' ', '\"', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '@', '[', ']', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '¢', '£', '©', '¬', 'Â', 'Ã', 'â', 'œ', 'š', 'ž', '‚', '„', '€']\n",
    "MAP_CHAR_IDX = {char: idx for idx, char in enumerate(CHARS_IN_TOYDATA, start=2)}\n",
    "\n",
    "def tokenize(hisco: str, max_len: int) -> list[int]:\n",
    "    encoded = [MAP_CHAR_IDX.get(char, 0) for char in hisco]\n",
    "    encoded = encoded[:max_len]\n",
    "    encoded += [1] * (max_len - len(encoded))\n",
    "\n",
    "    return encoded\n",
    "\n",
    "\n",
    "class HISCODataset(Dataset):\n",
    "    def __init__(self, dataset: pd.DataFrame):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = partial(tokenize, max_len=32)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, item: int) -> dict[str, str | Tensor]:\n",
    "        record = self.dataset.iloc[item]\n",
    "        encoded = self.tokenizer(record.occ1)\n",
    "\n",
    "        package = {\n",
    "            'occ1': record.occ1,\n",
    "            'encoded': torch.tensor(encoded, dtype=torch.long),\n",
    "            'label': torch.tensor(record.label, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "        return package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dirs import DATA_DIR\n",
    "\n",
    "train_data = pd.read_csv(os.path.join(DATA_DIR, 'toy_data_train.csv'))\n",
    "test_data = pd.read_csv(os.path.join(DATA_DIR, 'toy_data_test.csv'))\n",
    "\n",
    "train_dataset = HISCODataset(train_data)\n",
    "test_dataset = HISCODataset(test_data)\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=32)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) load in label <-> HISCO code mapping. Requires `histocc`\n",
    "from histocc import DATASETS\n",
    "\n",
    "keys = DATASETS['keys']()\n",
    "map_hisco_label = dict(keys[['hisco', 'code']].values)\n",
    "map_label_hisco = {v: k for k, v in map_hisco_label.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HISCOClassifierGRU(nn.Module):\n",
    "    def __init__(self, vocab_size: int = 100, hidden_size: int = 128):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.classifier = nn.Linear(hidden_size, 1919)\n",
    "\n",
    "    def forward(self, input_seq: Tensor) -> Tensor:\n",
    "        out = self.embedding(input_seq)\n",
    "        out, _ = self.gru(out)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.classifier(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_baseline = HISCOClassifierGRU()\n",
    "optimizer_baseline = torch.optim.AdamW(model_baseline.parameters(), lr=0.01)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, data_loader, loss_fn):\n",
    "    model.train()\n",
    "\n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out = model(batch['encoded']) # make predictions\n",
    "        loss = loss_fn(out, batch['label']) # calculate loss\n",
    "        loss.backward() # calculate derivatives\n",
    "\n",
    "        optimizer.step() # update network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "\n",
    "    total_correct = 0 # keep count of correct predictions\n",
    "    total_count = 0 # keep count of total number of predictions\n",
    "\n",
    "    for batch in data_loader:\n",
    "        out = model(batch['encoded']).argmax(1)\n",
    "\n",
    "        total_correct += (out == batch['label']).sum().item()\n",
    "        total_count += batch['label'].size(0)\n",
    "\n",
    "    return total_correct / total_count # calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained for 1 epochs. Validation accuracy: 64.0%\n",
      "Trained for 2 epochs. Validation accuracy: 70.8%\n",
      "Trained for 3 epochs. Validation accuracy: 73.2%\n",
      "Trained for 4 epochs. Validation accuracy: 72.3%\n",
      "Trained for 5 epochs. Validation accuracy: 72.6%\n",
      "Trained for 6 epochs. Validation accuracy: 74.1%\n",
      "Trained for 7 epochs. Validation accuracy: 75.1%\n",
      "Trained for 8 epochs. Validation accuracy: 76.4%\n",
      "Trained for 9 epochs. Validation accuracy: 76.8%\n",
      "Trained for 10 epochs. Validation accuracy: 76.6%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 11):\n",
    "    train_epoch(model_baseline, optimizer_baseline, train_data_loader, loss_fn)\n",
    "    acc = evaluate(model_baseline, test_data_loader)\n",
    "\n",
    "    print(f'Trained for {epoch} epochs. Validation accuracy: {100 * acc}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HISCOClassifierSimpleRNN(nn.Module):\n",
    "    def __init__(self, vocab_size: int = 100, hidden_size: int = 128):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rnn = nn.RNN(hidden_size, hidden_size, batch_first=True)\n",
    "        self.classifier = nn.Linear(hidden_size, 1919)\n",
    "\n",
    "    def forward(self, input_seq: Tensor) -> Tensor:\n",
    "        out = self.embedding(input_seq)\n",
    "        out, _ = self.rnn(out)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.classifier(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class HISCOClassifierLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size: int = 100, hidden_size: int = 128):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.classifier = nn.Linear(hidden_size, 1919)\n",
    "\n",
    "    def forward(self, input_seq: Tensor) -> Tensor:\n",
    "        out = self.embedding(input_seq)\n",
    "        out, _ = self.lstm(out)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.classifier(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_simple = HISCOClassifierSimpleRNN()\n",
    "optimizer_simple = torch.optim.AdamW(model_simple.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = HISCOClassifierLSTM()\n",
    "optimizer_lstm = torch.optim.AdamW(model_lstm.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained \"SimpleRNN\" model for 1 epochs. Validation accuracy: 9.4%\n",
      "Trained \"SimpleRNN\" model for 2 epochs. Validation accuracy: 10.100000000000001%\n",
      "Trained \"SimpleRNN\" model for 3 epochs. Validation accuracy: 10.299999999999999%\n",
      "Trained \"SimpleRNN\" model for 4 epochs. Validation accuracy: 10.299999999999999%\n",
      "Trained \"SimpleRNN\" model for 5 epochs. Validation accuracy: 10.4%\n",
      "Trained \"SimpleRNN\" model for 6 epochs. Validation accuracy: 9.2%\n",
      "Trained \"SimpleRNN\" model for 7 epochs. Validation accuracy: 9.3%\n",
      "Trained \"SimpleRNN\" model for 8 epochs. Validation accuracy: 9.4%\n",
      "Trained \"SimpleRNN\" model for 9 epochs. Validation accuracy: 9.3%\n",
      "Trained \"SimpleRNN\" model for 10 epochs. Validation accuracy: 9.3%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 11):\n",
    "    train_epoch(model_simple, optimizer_simple, train_data_loader, loss_fn)\n",
    "    acc = evaluate(model_simple, test_data_loader)\n",
    "\n",
    "    print(f'Trained \"SimpleRNN\" model for {epoch} epochs. Validation accuracy: {100 * acc}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained \"LSTM\" model for 1 epochs. Validation accuracy: 9.2%\n",
      "Trained \"LSTM\" model for 2 epochs. Validation accuracy: 14.000000000000002%\n",
      "Trained \"LSTM\" model for 3 epochs. Validation accuracy: 19.5%\n",
      "Trained \"LSTM\" model for 4 epochs. Validation accuracy: 25.6%\n",
      "Trained \"LSTM\" model for 5 epochs. Validation accuracy: 35.8%\n",
      "Trained \"LSTM\" model for 6 epochs. Validation accuracy: 40.8%\n",
      "Trained \"LSTM\" model for 7 epochs. Validation accuracy: 44.5%\n",
      "Trained \"LSTM\" model for 8 epochs. Validation accuracy: 45.6%\n",
      "Trained \"LSTM\" model for 9 epochs. Validation accuracy: 46.2%\n",
      "Trained \"LSTM\" model for 10 epochs. Validation accuracy: 47.4%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 11):\n",
    "    train_epoch(model_lstm, optimizer_lstm, train_data_loader, loss_fn)\n",
    "    acc = evaluate(model_lstm, test_data_loader)\n",
    "\n",
    "    print(f'Trained \"LSTM\" model for {epoch} epochs. Validation accuracy: {100 * acc}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm_new_optimizer = HISCOClassifierLSTM()\n",
    "optimizer_lstm_new_optimizer = torch.optim.RMSprop(model_lstm_new_optimizer.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained \"LSTM\" (RMSprop) model for 1 epochs. Validation accuracy: 18.7%\n",
      "Trained \"LSTM\" (RMSprop) model for 2 epochs. Validation accuracy: 27.500000000000004%\n",
      "Trained \"LSTM\" (RMSprop) model for 3 epochs. Validation accuracy: 41.9%\n",
      "Trained \"LSTM\" (RMSprop) model for 4 epochs. Validation accuracy: 46.7%\n",
      "Trained \"LSTM\" (RMSprop) model for 5 epochs. Validation accuracy: 51.7%\n",
      "Trained \"LSTM\" (RMSprop) model for 6 epochs. Validation accuracy: 55.900000000000006%\n",
      "Trained \"LSTM\" (RMSprop) model for 7 epochs. Validation accuracy: 58.099999999999994%\n",
      "Trained \"LSTM\" (RMSprop) model for 8 epochs. Validation accuracy: 59.8%\n",
      "Trained \"LSTM\" (RMSprop) model for 9 epochs. Validation accuracy: 61.8%\n",
      "Trained \"LSTM\" (RMSprop) model for 10 epochs. Validation accuracy: 63.2%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 11):\n",
    "    train_epoch(model_lstm_new_optimizer, optimizer_lstm_new_optimizer, train_data_loader, loss_fn)\n",
    "    acc = evaluate(model_lstm_new_optimizer, test_data_loader)\n",
    "\n",
    "    print(f'Trained \"LSTM\" (RMSprop) model for {epoch} epochs. Validation accuracy: {100 * acc}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FancyHISCOClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size: int = 100, hidden_size: int = 128, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.lstm = nn.GRU(\n",
    "            hidden_size, \n",
    "            hidden_size, \n",
    "            batch_first=True,\n",
    "            num_layers=2,\n",
    "            )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(hidden_size, 1919)\n",
    "\n",
    "    def forward(self, input_seq: Tensor) -> Tensor:\n",
    "        out = self.embedding(input_seq)\n",
    "        out, _ = self.lstm(out)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.dropout(out)\n",
    "        out = self.classifier(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "model_fancy = FancyHISCOClassifier(hidden_size=128, dropout=0.5).to(device)\n",
    "optimizer_fancy = torch.optim.AdamW(model_fancy.parameters(), lr=0.01)\n",
    "scheduler_fancy = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_fancy, T_max=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_select_device(model, optimizer, data_loader, loss_fn, device):\n",
    "    model.train()\n",
    "\n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out = model(batch['encoded'].to(device)) # make predictions\n",
    "        loss = loss_fn(out, batch['label'].to(device)) # calculate loss\n",
    "        loss.backward() # calculate derivatives\n",
    "\n",
    "        optimizer.step() # update network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def evaluate_select_device(model, data_loader, device):\n",
    "    model.eval()\n",
    "\n",
    "    total_correct = 0 # keep count of correct predictions\n",
    "    total_count = 0 # keep count of total number of predictions\n",
    "\n",
    "    for batch in data_loader:\n",
    "        out = model(batch['encoded'].to(device)).argmax(1).cpu()\n",
    "\n",
    "        total_correct += (out == batch['label']).sum().item()\n",
    "        total_count += batch['label'].size(0)\n",
    "\n",
    "    return total_correct / total_count # calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained \"fancy\" model for 1 epochs. Validation accuracy: 57.4%\n",
      "Trained \"fancy\" model for 2 epochs. Validation accuracy: 61.8%\n",
      "Trained \"fancy\" model for 3 epochs. Validation accuracy: 66.9%\n",
      "Trained \"fancy\" model for 4 epochs. Validation accuracy: 71.89999999999999%\n",
      "Trained \"fancy\" model for 5 epochs. Validation accuracy: 72.2%\n",
      "Trained \"fancy\" model for 6 epochs. Validation accuracy: 71.39999999999999%\n",
      "Trained \"fancy\" model for 7 epochs. Validation accuracy: 73.0%\n",
      "Trained \"fancy\" model for 8 epochs. Validation accuracy: 73.6%\n",
      "Trained \"fancy\" model for 9 epochs. Validation accuracy: 74.2%\n",
      "Trained \"fancy\" model for 10 epochs. Validation accuracy: 75.2%\n",
      "Trained \"fancy\" model for 11 epochs. Validation accuracy: 76.0%\n",
      "Trained \"fancy\" model for 12 epochs. Validation accuracy: 78.10000000000001%\n",
      "Trained \"fancy\" model for 13 epochs. Validation accuracy: 75.8%\n",
      "Trained \"fancy\" model for 14 epochs. Validation accuracy: 76.0%\n",
      "Trained \"fancy\" model for 15 epochs. Validation accuracy: 77.2%\n",
      "Trained \"fancy\" model for 16 epochs. Validation accuracy: 78.9%\n",
      "Trained \"fancy\" model for 17 epochs. Validation accuracy: 77.9%\n",
      "Trained \"fancy\" model for 18 epochs. Validation accuracy: 78.60000000000001%\n",
      "Trained \"fancy\" model for 19 epochs. Validation accuracy: 79.9%\n",
      "Trained \"fancy\" model for 20 epochs. Validation accuracy: 77.60000000000001%\n",
      "Trained \"fancy\" model for 21 epochs. Validation accuracy: 78.9%\n",
      "Trained \"fancy\" model for 22 epochs. Validation accuracy: 79.7%\n",
      "Trained \"fancy\" model for 23 epochs. Validation accuracy: 80.80000000000001%\n",
      "Trained \"fancy\" model for 24 epochs. Validation accuracy: 80.10000000000001%\n",
      "Trained \"fancy\" model for 25 epochs. Validation accuracy: 81.3%\n",
      "Trained \"fancy\" model for 26 epochs. Validation accuracy: 80.0%\n",
      "Trained \"fancy\" model for 27 epochs. Validation accuracy: 81.8%\n",
      "Trained \"fancy\" model for 28 epochs. Validation accuracy: 80.9%\n",
      "Trained \"fancy\" model for 29 epochs. Validation accuracy: 80.7%\n",
      "Trained \"fancy\" model for 30 epochs. Validation accuracy: 82.1%\n",
      "Trained \"fancy\" model for 31 epochs. Validation accuracy: 83.1%\n",
      "Trained \"fancy\" model for 32 epochs. Validation accuracy: 82.6%\n",
      "Trained \"fancy\" model for 33 epochs. Validation accuracy: 83.39999999999999%\n",
      "Trained \"fancy\" model for 34 epochs. Validation accuracy: 82.6%\n",
      "Trained \"fancy\" model for 35 epochs. Validation accuracy: 83.7%\n",
      "Trained \"fancy\" model for 36 epochs. Validation accuracy: 83.7%\n",
      "Trained \"fancy\" model for 37 epochs. Validation accuracy: 83.7%\n",
      "Trained \"fancy\" model for 38 epochs. Validation accuracy: 84.5%\n",
      "Trained \"fancy\" model for 39 epochs. Validation accuracy: 84.2%\n",
      "Trained \"fancy\" model for 40 epochs. Validation accuracy: 84.1%\n",
      "Trained \"fancy\" model for 41 epochs. Validation accuracy: 83.89999999999999%\n",
      "Trained \"fancy\" model for 42 epochs. Validation accuracy: 84.5%\n",
      "Trained \"fancy\" model for 43 epochs. Validation accuracy: 84.6%\n",
      "Trained \"fancy\" model for 44 epochs. Validation accuracy: 84.39999999999999%\n",
      "Trained \"fancy\" model for 45 epochs. Validation accuracy: 84.39999999999999%\n",
      "Trained \"fancy\" model for 46 epochs. Validation accuracy: 84.7%\n",
      "Trained \"fancy\" model for 47 epochs. Validation accuracy: 84.2%\n",
      "Trained \"fancy\" model for 48 epochs. Validation accuracy: 84.2%\n",
      "Trained \"fancy\" model for 49 epochs. Validation accuracy: 84.39999999999999%\n",
      "Trained \"fancy\" model for 50 epochs. Validation accuracy: 84.39999999999999%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 51):\n",
    "    train_epoch_select_device(model_fancy, optimizer_fancy, train_data_loader, loss_fn, device)\n",
    "    acc = evaluate_select_device(model_fancy, test_data_loader, device)\n",
    "\n",
    "    print(f'Trained \"fancy\" model for {epoch} epochs. Validation accuracy: {100 * acc}%')\n",
    "\n",
    "    scheduler_fancy.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hedg-summer-school",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
