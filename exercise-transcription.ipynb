{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Transcription\n",
    "\n",
    "1. Try experimenting with an alternative (non-linear) activation function to the `sigmoid` function used in the baseline model. A list of activation functions is available [here](https://pytorch.org/docs/stable/nn.functional.html#non-linear-activation-functions)\n",
    "1. Try changing the number of channels in the first convolutional layer (`self.conv1`) from 16 to 32. What else do you need to change to ensure the model still work? How does this affect the number of parameters of the model?\n",
    "1. Experiment with, e.g., the numbers of layers in your model (**note**: this may necessitate appropriately asjusting the subsequent layers), the choice of optimizer and learning rate, or the number of epochs. How high performance can you achieve on the test split?\n",
    "\n",
    "To get you started, run the code in the [A baseline model](#a-baseline-model)-section of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch import Tensor, nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transforms = transforms.ToTensor() # convert to Tensors our models can work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(root='path/to/where/you/want/to/store/data', train=True, download=True, transform=image_transforms)\n",
    "test_dataset = datasets.MNIST(root='path/to/where/you/want/to/store/data', train=False, download=True, transform=image_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = DataLoader(train_dataset, batch_size=32)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Let's take a look*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5, 5, i + 1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_dataset[i][0][0], cmap='gray')\n",
    "    plt.xlabel(f'Label: {train_dataset[i][1]}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandwrittenDigitsRecognizer(nn.Module):\n",
    "    '''\n",
    "    Baseline handwritten digits neural network. Note that the\n",
    "    dimensions of the intermediate states; see the comments\n",
    "    in the `forward` module, which shows all dimensions during\n",
    "    the forward pass. Note that the initial dimensions of our\n",
    "    images is (1, 28, 28), which means a greyscale (hence the 1)\n",
    "    images 28 pixels high and wide.\n",
    "\n",
    "    The changes in dimensions occur for the following reasons:\n",
    "    1)  With a 3x3 kernel, we lose 1 pixels on the edges, which\n",
    "        changes the heigh and width from 28x28 to 26x26. We also\n",
    "        change the number of channels from 1 to 16.\n",
    "    2)  Using pooling with a kernel size of 2x2 and strides 2x2,\n",
    "        we change each 4x4 block to a 1x1 block, which changes the\n",
    "        heigh and width from 26x26 to 13x13 (one fourth)\n",
    "    3)  With a 3x3 kernel, we lose 1 pixels on the edges, which\n",
    "        changes the heigh and width from 13x13 to 11x11. We also\n",
    "        change the number of channels from 16 to 32.\n",
    "    4)  Using pooling with a kernel size of 2x2 and strides 2x2,\n",
    "        we change each 4x4 block to a 1x1 block, which changes the\n",
    "        heigh and width from 11x11 to 5x5 (one fourth, where we also\n",
    "        lose some pixels as 11 isn't divisible by 2).\n",
    "    5)  Flattening refers to collapsing our representation from three\n",
    "        to dimension, and since 5*5*32 = 800, we end up with 800 elements\n",
    "        in the resulting vector.\n",
    "    6)  Finally, we want to reduce the dimensions to 10, as we have ten\n",
    "        distinct classes (the numbers 0-9).\n",
    "\n",
    "    If you want to change, e.g., the number of channels in one of the\n",
    "    convolutional layers, you need to also consider how this affects\n",
    "    the subsequent layer.\n",
    "\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, 1) # from 1 to 16 channels, 3x3 kernels, stride = 1\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, 1) # from 16 to 32 channels, 3x3 kernels, stride = 1\n",
    "        self.classifier = nn.Linear(800, 10) # from 800 to 10-way classification (0-9)\n",
    "\n",
    "    def forward(self, image: Tensor) -> Tensor:\n",
    "        out = self.conv1(image) # (1, 28, 28) -> (16, 26, 26)\n",
    "        out = torch.sigmoid(out)\n",
    "        out = nn.functional.max_pool2d(out, 2) # (16, 26, 26) -> (16, 13, 13)\n",
    "\n",
    "        out = self.conv2(out) # (16, 13, 13) -> (32, 11, 11)\n",
    "        out = torch.sigmoid(out)\n",
    "        out = nn.functional.max_pool2d(out, 2) # (32, 11, 11) -> (32, 5, 5)\n",
    "\n",
    "        out = torch.flatten(out, 1) # (32, 5, 5) -> (800,)\n",
    "        out = self.classifier(out) # (800,) -> (10,)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We can reuse our training loop from earlier...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, data_loader, loss_fn):\n",
    "    model.train()\n",
    "\n",
    "    for image, label in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out = model(image) # make predictions\n",
    "        loss = loss_fn(out, label) # calculate loss\n",
    "        loss.backward() # calculate derivatives\n",
    "\n",
    "        optimizer.step() # update network parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*... as well as our evaluation loop*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "\n",
    "    total_correct = 0 # keep count of correct predictions\n",
    "    total_count = 0 # keep count of total number of predictions\n",
    "\n",
    "    for image, label in data_loader:\n",
    "        out = model(image).argmax(1)\n",
    "\n",
    "        total_correct += (out == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "\n",
    "    return total_correct / total_count # calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_baseline = HandwrittenDigitsRecognizer()\n",
    "optimizer_baseline = torch.optim.SGD(model_baseline.parameters(), lr=0.01)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of parameters: {sum(p.numel() for p in model_baseline.parameters()):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, 11):\n",
    "    train_epoch(model_baseline, optimizer_baseline, train_data_loader, loss_fn)\n",
    "    acc = evaluate(model_baseline, test_data_loader)\n",
    "\n",
    "    print(f'Trained for {epoch} epochs. Validation accuracy: {100 * acc}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5, 5, i + 1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "\n",
    "    image, label = test_dataset[i]\n",
    "    pred = model_baseline(image.unsqueeze(0)).argmax()\n",
    "    \n",
    "    plt.imshow(image[0], cmap='gray')    \n",
    "    plt.xlabel(f'Label/pred: {label}/{pred}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandwrittenDigitsRecognizerOtherActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, 1) # from 1 to 3 channels, 3x3 kernels, stride = 1\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, 1) # from 16 to 32 channels, 3x3 kernels, stride = 1\n",
    "        self.classifier = nn.Linear(800, 10) # from 800 to 10-way classification (0-9)\n",
    "\n",
    "    def forward(self, image: Tensor) -> Tensor:\n",
    "        out = self.conv1(image) # (1, 28, 28) -> (16, 26, 26)\n",
    "        out = torch.??(out)\n",
    "        out = nn.functional.max_pool2d(out, 2) # (16, 26, 26) -> (16, 13, 13)\n",
    "\n",
    "        out = self.conv2(out) # (16, 13, 13) -> (32, 11, 11)\n",
    "        out = torch.??(out)\n",
    "        out = nn.functional.max_pool2d(out, 2) # (32, 11, 11) -> (32, 5, 5)\n",
    "\n",
    "        out = torch.flatten(out, 1) # (32, 5, 5) -> (800,)\n",
    "        out = self.classifier(out) # (800,) -> (10,)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_relu = HandwrittenDigitsRecognizerOtherActivation()\n",
    "optimizer_relu = torch.optim.SGD(model_relu.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, 11):\n",
    "    train_epoch(model_relu, optimizer_relu, train_data_loader, loss_fn)\n",
    "    acc = evaluate(model_relu, test_data_loader)\n",
    "\n",
    "    print(f'Trained for {epoch} epochs. Validation accuracy: {100 * acc}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandwrittenDigitsRecognizerLarger(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, ??, 3, 1) # from 1 to 32 channels, 3x3 kernels, stride = 1\n",
    "        self.conv2 = nn.Conv2d(??, 32, 3, 1) # from 32 to 32 channels, 3x3 kernels, stride = 1\n",
    "        self.classifier = nn.Linear(800, 10) # from 800 to 10-way classification (0-9)\n",
    "\n",
    "    def forward(self, image: Tensor) -> Tensor:\n",
    "        out = self.conv1(image) # (1, 28, 28) -> (16, 26, 26)\n",
    "        out = torch.??(out)\n",
    "        out = nn.functional.max_pool2d(out, 2) # (16, 26, 26) -> (16, 13, 13)\n",
    "\n",
    "        out = self.conv2(out) # (16, 13, 13) -> (32, 11, 11)\n",
    "        out = torch.??(out)\n",
    "        out = nn.functional.max_pool2d(out, 2) # (32, 11, 11) -> (32, 5, 5)\n",
    "\n",
    "        out = torch.flatten(out, 1) # (32, 5, 5) -> (800,)\n",
    "        out = self.classifier(out) # (800,) -> (10,)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_large = HandwrittenDigitsRecognizerLarger()\n",
    "optimizer_large = torch.optim.SGD(model_large.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of parameters: {sum(p.numel() for p in model_large.parameters()):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, 11):\n",
    "    train_epoch(model_large, optimizer_large, train_data_loader, loss_fn)\n",
    "    acc = evaluate(model_large, test_data_loader)\n",
    "\n",
    "    print(f'Trained for {epoch} epochs. Validation accuracy: {100 * acc}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandwrittenDigitsRecognizerReLUDeeper(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, 1) # from 1 to 16 channels, 3x3 kernels, stride = 1\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, 1) # from 16 to 32 channels, 3x3 kernels, stride = 1\n",
    "        self.conv3 = ??\n",
    "        self.classifier = nn.Linear(??, 10) # from 800 to 10-way classification (0-9)\n",
    "\n",
    "    def forward(self, image: Tensor) -> Tensor:\n",
    "        out = self.conv1(image) # (1, 28, 28) -> (16, 26, 26)\n",
    "        out = torch.relu(out)\n",
    "        out = self.conv2(out) # (1, 26, 26) -> (16, 24, 24)\n",
    "        out = torch.relu(out)\n",
    "        out = nn.functional.max_pool2d(out, 2) # (16, 24, 24) -> (16, 12, 12)\n",
    "\n",
    "        out = self.conv3(out) # (16, 12, 12) -> ??\n",
    "        out = torch.relu(out)\n",
    "        out = nn.functional.max_pool2d(out, 2) # ?? -> ??\n",
    "\n",
    "        out = torch.flatten(out, 1) # ?? -> ??\n",
    "        out = self.classifier(out) # ?? -> (10,)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_deep = HandwrittenDigitsRecognizerReLUDeeper()\n",
    "optimizer_deep = torch.optim.SGD(model_deep.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of parameters: {sum(p.numel() for p in model_deep.parameters()):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, 11):\n",
    "    train_epoch(model_deep, optimizer_deep, train_data_loader, loss_fn)\n",
    "    acc = evaluate(model_deep, test_data_loader)\n",
    "\n",
    "    print(f'Trained for {epoch} epochs. Validation accuracy: {100 * acc}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hedg-summer-school",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
