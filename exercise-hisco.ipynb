{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Classification of Occupational Descriptions\n",
    "\n",
    "1. Reproduce the HISCO classification model from the slides by downloading and loading the datasets and running the associated code (see [Exercise 1](##exercise-1))\n",
    "1. Try changing the `nn.GRU` layer to an `nn.RNN` layer and then to an `nn.LSTM` layer and train your new models\n",
    "1. Try changing the optimizer to something other than `torch.optim.AdamW`. A list of optimizers is available [here](https://pytorch.org/docs/stable/optim.html#algorithms)\n",
    "1. Experiment with, e.g., the types or numbers of layers in your model, the choice of optimizer and learning rate, or the number of epochs. How high performance can you achieve on the test split?\n",
    "\n",
    "## Data\n",
    "\n",
    "Start by downloading the datasets `toy_data_train.csv` and `toy_data_test.csv`.\n",
    "\n",
    "Alternatively, if you have `histocc` installed, you can run the code below to prepare the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from histocc import DATASETS\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def download_and_prepare_data():\n",
    "    keys = DATASETS['keys']()\n",
    "    mapping = dict(keys[['hisco', 'code']].values)\n",
    "\n",
    "    toydata = DATASETS['toydata']()\n",
    "    toydata['label'] = toydata['hisco_1'].transform(lambda x: mapping[x])\n",
    "\n",
    "    train, test = train_test_split(\n",
    "        toydata[['occ1', 'label']],\n",
    "        test_size=0.1,\n",
    "        random_state=42,\n",
    "        )\n",
    "\n",
    "    train.to_csv('./toy_data_train.csv', index=False)\n",
    "    test.to_csv('./toy_data_test.csv', index=False)\n",
    "\n",
    "\n",
    "download_and_prepare_data()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch import Tensor, nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Dataset` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all the individual tokens present in the toy dataset\n",
    "CHARS_IN_TOYDATA = [' ', '\"', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '@', '[', ']', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '¢', '£', '©', '¬', 'Â', 'Ã', 'â', 'œ', 'š', 'ž', '‚', '„', '€']\n",
    "MAP_CHAR_IDX = {char: idx for idx, char in enumerate(CHARS_IN_TOYDATA, start=2)}\n",
    "\n",
    "def tokenize(hisco: str, max_len: int) -> list[int]:\n",
    "    encoded = [MAP_CHAR_IDX.get(char, 0) for char in hisco]\n",
    "    encoded = encoded[:max_len]\n",
    "    encoded += [1] * (max_len - len(encoded))\n",
    "\n",
    "    return encoded\n",
    "\n",
    "\n",
    "class HISCODataset(Dataset):\n",
    "    def __init__(self, dataset: pd.DataFrame):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = partial(tokenize, max_len=32)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, item: int) -> dict[str, str | Tensor]:\n",
    "        record = self.dataset.iloc[item]\n",
    "        encoded = self.tokenizer(record.occ1)\n",
    "\n",
    "        package = {\n",
    "            'occ1': record.occ1,\n",
    "            'encoded': torch.tensor(encoded, dtype=torch.long),\n",
    "            'label': torch.tensor(record.label, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "        return package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Make sure to adjust file paths\n",
    "train_data = pd.read_csv('path/to/toy_data_train.csv')\n",
    "test_data = pd.read_csv('path/to/toy_data_test.csv')\n",
    "\n",
    "train_dataset = HISCODataset(train_data)\n",
    "test_dataset = HISCODataset(test_data)\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=32)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) load in label <-> HISCO code mapping. Requires `histocc`\n",
    "from histocc import DATASETS\n",
    "\n",
    "keys = DATASETS['keys']()\n",
    "map_hisco_label = dict(keys[['hisco', 'code']].values)\n",
    "map_label_hisco = {v: k for k, v in map_hisco_label.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HISCOClassifierGRU(nn.Module):\n",
    "    def __init__(self, vocab_size: int = 100, hidden_size: int = 128):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.classifier = nn.Linear(hidden_size, 1919)\n",
    "\n",
    "    def forward(self, input_seq: Tensor) -> Tensor:\n",
    "        out = self.embedding(input_seq)\n",
    "        out, _ = self.gru(out)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.classifier(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_baseline = HISCOClassifierGRU()\n",
    "optimizer_baseline = torch.optim.AdamW(model_baseline.parameters(), lr=0.01)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, data_loader, loss_fn):\n",
    "    model.train()\n",
    "\n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out = model(batch['encoded']) # make predictions\n",
    "        loss = loss_fn(out, batch['label']) # calculate loss\n",
    "        loss.backward() # calculate derivatives\n",
    "\n",
    "        optimizer.step() # update network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "\n",
    "    total_correct = 0 # keep count of correct predictions\n",
    "    total_count = 0 # keep count of total number of predictions\n",
    "\n",
    "    for batch in data_loader:\n",
    "        out = model(batch['encoded']).argmax(1)\n",
    "\n",
    "        total_correct += (out == batch['label']).sum().item()\n",
    "        total_count += batch['label'].size(0)\n",
    "\n",
    "    return total_correct / total_count # calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, 11):\n",
    "    train_epoch(model_baseline, optimizer_baseline, train_data_loader, loss_fn)\n",
    "    acc = evaluate(model_baseline, test_data_loader)\n",
    "\n",
    "    print(f'Trained for {epoch} epochs. Validation accuracy: {100 * acc}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HISCOClassifierSimpleRNN(nn.Module):\n",
    "    def __init__(self, vocab_size: int = 100, hidden_size: int = 128):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rnn = nn.??(hidden_size, hidden_size, batch_first=True)\n",
    "        self.classifier = nn.Linear(hidden_size, 1919)\n",
    "\n",
    "    def forward(self, input_seq: Tensor) -> Tensor:\n",
    "        out = self.embedding(input_seq)\n",
    "        out, _ = self.rnn(out)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.classifier(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class HISCOClassifierLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size: int = 100, hidden_size: int = 128):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.lstm = nn.??(hidden_size, hidden_size, batch_first=True)\n",
    "        self.classifier = nn.Linear(hidden_size, 1919)\n",
    "\n",
    "    def forward(self, input_seq: Tensor) -> Tensor:\n",
    "        out = self.embedding(input_seq)\n",
    "        out, _ = self.lstm(out)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.classifier(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_simple = HISCOClassifierSimpleRNN()\n",
    "optimizer_simple = torch.optim.AdamW(model_simple.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = HISCOClassifierLSTM()\n",
    "optimizer_lstm = torch.optim.AdamW(model_lstm.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, 11):\n",
    "    train_epoch(model_simple, optimizer_simple, train_data_loader, loss_fn)\n",
    "    acc = evaluate(model_simple, test_data_loader)\n",
    "\n",
    "    print(f'Trained \"SimpleRNN\" model for {epoch} epochs. Validation accuracy: {100 * acc}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, 11):\n",
    "    train_epoch(??)\n",
    "    acc = ??\n",
    "\n",
    "    print(f'Trained \"LSTM\" model for {epoch} epochs. Validation accuracy: {100 * acc}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm_new_optimizer = HISCOClassifierLSTM()\n",
    "optimizer_lstm_new_optimizer = torch.optim.??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, 11):\n",
    "    train_epoch(??)\n",
    "    acc = ??\n",
    "\n",
    "    print(f'Trained \"LSTM\" (RMSprop) model for {epoch} epochs. Validation accuracy: {100 * acc}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FancyHISCOClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size: int = 100, hidden_size: int = 128, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = ??\n",
    "        self.?? = nn.?? # your recurrent layer(s) here\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = ??\n",
    "\n",
    "    def forward(self, input_seq: Tensor) -> Tensor:\n",
    "        out = self.embedding(input_seq)\n",
    "        out, _ = self.??(out)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.dropout(out)\n",
    "        out = self.classifier(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu' # if GPU available, change this to 'cuda' for faster training\n",
    "\n",
    "model_fancy = FancyHISCOClassifier(hidden_size=??, dropout=0.??).to(device)\n",
    "optimizer_fancy = torch.optim.??(model_fancy.parameters(), lr=??)\n",
    "scheduler_fancy = torch.optim.lr_scheduler.??(optimizer_fancy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_select_device(model, optimizer, data_loader, loss_fn, device):\n",
    "    model.train()\n",
    "\n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out = model(batch['encoded'].to(device)) # make predictions\n",
    "        loss = loss_fn(out, batch['label'].to(device)) # calculate loss\n",
    "        loss.backward() # calculate derivatives\n",
    "\n",
    "        optimizer.step() # update network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def evaluate_select_device(model, data_loader, device):\n",
    "    model.eval()\n",
    "\n",
    "    total_correct = 0 # keep count of correct predictions\n",
    "    total_count = 0 # keep count of total number of predictions\n",
    "\n",
    "    for batch in data_loader:\n",
    "        out = model(batch['encoded'].to(device)).argmax(1).cpu()\n",
    "\n",
    "        total_correct += (out == batch['label']).sum().item()\n",
    "        total_count += batch['label'].size(0)\n",
    "\n",
    "    return total_correct / total_count # calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, ??):\n",
    "    train_epoch_select_device(model_fancy, optimizer_fancy, train_data_loader, loss_fn, device)\n",
    "    acc = evaluate_select_device(model_fancy, test_data_loader, device)\n",
    "\n",
    "    print(f'Trained \"fancy\" model for {epoch} epochs. Validation accuracy: {100 * acc}%')\n",
    "\n",
    "    scheduler_fancy.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
